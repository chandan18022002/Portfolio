{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a831d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edd8f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class envoirnment:\n",
    "    def __init__(self,observation_space_n,action_space_n,state_space,reward_space,state_space_width,state_space_length):\n",
    "        self.observation_space_n=observation_space_n\n",
    "        self.action_space_n=action_space_n\n",
    "        self.state_space=state_space\n",
    "        self.reward_space=reward_space\n",
    "        self.state_space_width=state_space_width\n",
    "        self.state_space_length=state_space_length\n",
    "\n",
    "    def __del__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21677676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space_n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space_n):\n",
    "            p[action] = 1 / env.action_space_n\n",
    "        policy[key] = p\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12a8aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "         Q[key] = {a: 0.0 for a in range(0, env.action_space_n)}\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66be708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(env, policy, display=True):\n",
    "    # env.reset()\n",
    "    state = env.state_space.index('S')\n",
    "    episode = []\n",
    "    finished = False\n",
    "\n",
    "    while not finished:\n",
    "        s = state\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "                top_range += prob[1]\n",
    "                if n < top_range:\n",
    "                    action = prob[0]\n",
    "                    break \n",
    "\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        state_x=state%env.state_space_width\n",
    "        state_y=state//env.state_space_width\n",
    "        if(action==0):\n",
    "            if (state_y>0):\n",
    "                state_y-=1\n",
    "        elif(action==1):\n",
    "            if (state_x<env.state_space_width-1):\n",
    "                state_x+=1\n",
    "        elif(action==2):\n",
    "            if (state_y<env.state_space_length-1):\n",
    "                state_y+=1\n",
    "        elif(action==3):\n",
    "            if (state_x>0):\n",
    "                state_x-=1\n",
    "        new_state=state_x+state_y*env.state_space_width\n",
    "        reward=env.reward_space[new_state]\n",
    "        if(env.state_space[new_state]=='G' or env.state_space[new_state]=='W'):\n",
    "            finished=True\n",
    "        else:\n",
    "            finished=False\n",
    "        state= new_state\n",
    "        # state, reward, finished,terminated, info = env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "\n",
    "        episode.append(timestep)\n",
    "        # if display:\n",
    "        #     # clear_output(True)\n",
    "        #     print(state_x,state_y)\n",
    "        #     # env.render()\n",
    "        #     # sleep(1)\n",
    "\n",
    "    # if display:\n",
    "    #     # clear_output(True)\n",
    "    #     # env.render()\n",
    "    #     print('state_x,state_y')\n",
    "    #     # sleep(1)\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c205edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, env,r=100):\n",
    "    wins = 0\n",
    "    episodes=[]\n",
    "    for i in range(r):\n",
    "        temp=run_game(env, policy, display=True)\n",
    "        episodes.append(temp)\n",
    "        w = temp[-1][-1]\n",
    "        # print(w)\n",
    "        if w == 1:\n",
    "                wins += 1\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0137cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
    "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {}\n",
    "    \n",
    "    for _ in range(episodes): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        \n",
    "        for i in reversed(range(0, len(episode))):   \n",
    "            s_t, a_t, r_t = episode[i] \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q\n",
    "                \n",
    "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f77c4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('FrozenLake8x8-v1')\n",
    "action_space = [0,1,2,3]#up,right,down,left,nothing\n",
    "state_space_width=20 # S is start, T is tile, W is Wall, G is Goal\n",
    "state_space_length=20\n",
    "state_space = ['T','T','T','T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','W',\n",
    "               'T','T','T','T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','G',\n",
    "               'T','T','T','T','T','T','T','T','W','T','T','T','T','T','T','T','T','W','T','T',\n",
    "               'T','T','T','T','T','T','T','W','W','T','W','T','T','T','T','T','W','T','T','T',\n",
    "               'T','T','T','T','T','T','W','T','T','T','W','T','T','T','T','W','T','T','T','T',\n",
    "               'T','T','T','T','T','T','W','T','W','T','W','T','T','T','W','T','T','T','T','T',\n",
    "               'T','T','T','T','T','T','W','T','W','T','W','T','T','W','T','T','T','T','T','T',\n",
    "               'T','W','T','T','T','T','W','T','W','T','W','T','W','T','T','T','T','T','T','T',\n",
    "               'T','W','T','T','T','T','T','T','W','T','T','W','T','T','T','T','T','T','T','T',\n",
    "               'S','W','T','T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','W','T','T','T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','W','T','T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','W','T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','T','T','T','W','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','T','T','W','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'T','W','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T',\n",
    "               'W','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T','T']\n",
    "state_space_width=5 # S is start, T is tile, W is Wall, G is Goal\n",
    "state_space_length=5\n",
    "state_space = ['S','T','T','T','T',\n",
    "               'T','T','T','T','T',\n",
    "               'T','W','W','T','T',\n",
    "               'T','T','T','T','T',\n",
    "               'T','T','T','T','G']\n",
    "# for i in range(len(state_space)):\n",
    "#     if not (state_space[i]=='S' or state_space[i]=='G'):\n",
    "#         if(random.randint(0,8)==0):\n",
    "#             state_space[i]='W'\n",
    "#         else:\n",
    "#             state_space[i]='T'\n",
    "# print(len(state_space))\n",
    "reward_space = []\n",
    "for i in state_space:\n",
    "    if(i=='S'):\n",
    "        reward_space.append(-1)\n",
    "    elif(i=='G'):\n",
    "        reward_space.append(10)\n",
    "    elif(i=='T'):\n",
    "        reward_space.append(-1)\n",
    "    elif(i=='W'):\n",
    "        reward_space.append(-100)\n",
    "# print('  Reward matrix')\n",
    "# for i in range(len(reward_space)):\n",
    "#     print('{:4}'.format(reward_space[i]),end = ',')\n",
    "#     if(i%state_space_width==state_space_width-1):\n",
    "#         print()\n",
    "\n",
    "observation_space_n=len(state_space)\n",
    "action_space_n=len(action_space)\n",
    "state_space\n",
    "reward_space\n",
    "state_space_width\n",
    "state_space_length\n",
    "env=envoirnment(observation_space_n,action_space_n,state_space,reward_space,state_space_width,state_space_length)\n",
    "policy = monte_carlo_e_soft(env, episodes=10000)\n",
    "# for i in policy:\n",
    "#     print(i,policy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a408abd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   A   \n",
      "T   T   T   T   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   T   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 8\n",
      "A   A   A   A   T   \n",
      "T   A   T   A   T   \n",
      "T   W   W   A   T   \n",
      "T   T   T   A   A   \n",
      "T   T   T   T   G   \n",
      "Number of steps 10\n"
     ]
    }
   ],
   "source": [
    "episodes=test_policy(policy, env,1)\n",
    "for j in episodes:\n",
    "    steps_taken=[]\n",
    "    for i in j:\n",
    "        # print(i[0]%state_space_width,i[0]//state_space_width)\n",
    "        steps_taken.append(i[0])\n",
    "\n",
    "    for i in range(len(state_space)):\n",
    "        if(i not in steps_taken):\n",
    "            print('{:3}'.format(state_space[i]),end = ' ')\n",
    "        else:\n",
    "            if(state_space[i] == 'G'):\n",
    "                print('{:3}'.format('A_G'),end = ' ')\n",
    "            else:\n",
    "                print('{:3}'.format('A'),end = ' ')\n",
    "        if(i%state_space_width==state_space_width-1):\n",
    "            print()\n",
    "    # We print the number of step it took.\n",
    "    print(\"Number of steps\", len(steps_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c753f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
